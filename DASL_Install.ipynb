{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog, iam, sql, settings\n",
    "from databricks.sdk.errors.platform import NotFound\n",
    "\n",
    "from dasl_client.client import Client\n",
    "from dasl_client.types import (\n",
    "    AdminConfig,\n",
    "    DatasourcesConfig,\n",
    "    DetectionRuleMetadata,\n",
    "    SystemTablesConfig,\n",
    "    WorkspaceConfig,\n",
    "    WorkspaceConfigObservables\n",
    ")\n",
    "import dasl_api\n",
    "\n",
    "service_principal_name = \"dasl-service-principal\"\n",
    "\n",
    "# STEP 1: Insert the Client ID as a string\n",
    "# Creating an app client ID requires account-level admin permissions. To\n",
    "# create an ID, navigate to the account management console -> settings ->\n",
    "# app connections and click \"Add Connection\". Use the following settings.\n",
    "# Application Name = \"Databricks Security Lakehouse\"\n",
    "# Redirect URLs = [\"https://api.sl.us-east-1.cloud.databricks.com/apis/dbui/v1/token-exchange\"]\n",
    "# Access Scopes = All APIs\n",
    "# Client secret: uncheck \"generate a client secret\"\n",
    "# Access token TTL = 60 (suggested)\n",
    "# Refresh token TTL = 10080 (suggested)\n",
    "# Save the app connection and populate the resulting client ID below.\n",
    "app_client_id = None\n",
    "if app_client_id is None:\n",
    "    raise Exception(\"must specify app_client_id\")\n",
    "\n",
    "# STEP 2: Specify the catalog where the data and metadata will be stored,\n",
    "# automatically managed by the security lakehouse.\n",
    "# You may use any name you wish here, and it is\n",
    "# also acceptable to specify a catalog which already exists.\n",
    "catalog_name = \"sec_lakehouse\"\n",
    "\n",
    "# STEP 3: Specify the managed location where catalogs will be created for \n",
    "# the lakehouse. Note that you will need to be a metastore admin for your\n",
    "# Databricks account; it is not generally sufficient to be a workspace\n",
    "# admin. If the catalog (catalog_name) above already exists, you may\n",
    "# leave this set to None.\n",
    "managed_location = None\n",
    "\n",
    "# Various details about the current user and workspace. Note in particular\n",
    "# that the current user's email address will be registered as the admin\n",
    "# email address for the Antimatter workspace created below\n",
    "dbc = WorkspaceClient()\n",
    "all_users = \"account users\"\n",
    "current_user = dbc.current_user.me().user_name\n",
    "databricks_instance = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "workspace = databricks_instance\n",
    "\n",
    "# Run the rest of the notebook as is to complete the installation.\n",
    "# The below cells contain configuration code can optionally be customized.\n",
    "# Reach out to support@antimatter.io for questions or assistance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a service principal. This service principal is used to manage\n",
    "# the catalogs created in this file. It will be granted (along with all\n",
    "# other account users) ALL_PRIVILEGES on the catalog(s) specified above,\n",
    "# but no additional privileges will be granted.\n",
    "sps = list(dbc.service_principals.list(\n",
    "    filter=f\"displayName eq '{service_principal_name}'\"\n",
    "))\n",
    "if len(sps) > 1:\n",
    "    raise ValueError(f\"multiple service principals found with name {service_principal_name}. Please pick a different name\")\n",
    "elif len(sps) == 1:\n",
    "    sp = sps[0]\n",
    "else:\n",
    "    sp = dbc.service_principals.create(display_name=service_principal_name)\n",
    "\n",
    "service_principal_application_id = sp.application_id\n",
    "service_principal_id = sp.id\n",
    "\n",
    "# Create an oauth secret for the service principal. This token will be sent\n",
    "# to the ASL control plane as a persistent means of authenticating with the\n",
    "# Databricks API when creating jobs.\n",
    "# Start by deleting the oldest existing secret (if necessary) for this\n",
    "# service principal since no more than 5 secrets are allowed.\n",
    "service_principal_secrets = dbc.api_client.do(\n",
    "    \"GET\",\n",
    "    path=f\"/api/2.0/accounts/servicePrincipals/{service_principal_id}/credentials/secrets\",\n",
    ")\n",
    "\n",
    "# Only delete a secret if there are already 5\n",
    "if \"secrets\" in service_principal_secrets and len(service_principal_secrets[\"secrets\"]) >= 5:\n",
    "    # Find the element with the oldest create_time\n",
    "    oldest = min(\n",
    "        service_principal_secrets[\"secrets\"],\n",
    "        key=lambda secret: secret[\"create_time\"]\n",
    "    )\n",
    "\n",
    "    dbc.api_client.do(\n",
    "        \"DELETE\",\n",
    "        path=f\"/api/2.0/accounts/servicePrincipals/{service_principal_id}/credentials/secrets/{oldest['id']}\",\n",
    "    )\n",
    "\n",
    "service_principal_secret = dbc.api_client.do(\n",
    "    \"POST\",\n",
    "    path=f\"/api/2.0/accounts/servicePrincipals/{service_principal_id}/credentials/secrets\",\n",
    ")[\"secret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the catalog (if required), the admin schema, and do the permission grants\n",
    "\n",
    "# Create the catalog, if it doesn't already exist\n",
    "try:\n",
    "    _ = dbc.catalogs.get(catalog_name)\n",
    "except NotFound:\n",
    "    # If the catalog did not already exist, we can create it for you, but we need to know what\n",
    "    # managed location to use\n",
    "    if managed_location is None:\n",
    "        raise Exception(\"must specify managed_location\")\n",
    "    dbc.catalogs.create(catalog_name, storage_root=managed_location)\n",
    "\n",
    "# Grant ALL_PRIVILEGES on the newly created catalog to the service principal and additionally to\n",
    "# all workspace users.\n",
    "# You can refine the all users grant later in the normal Unity Catalog UI, but this is a good default\n",
    "for principal in [service_principal_application_id, all_users]:\n",
    "    grants = dbc.grants.get(\n",
    "        securable_type=catalog.SecurableType.CATALOG.name,\n",
    "        full_name=catalog_name,\n",
    "        principal=principal,\n",
    "    ).privilege_assignments\n",
    "\n",
    "    create_grant = True\n",
    "    for pa in grants:\n",
    "        if catalog.Privilege.ALL_PRIVILEGES in pa.privileges:\n",
    "            create_grant = False\n",
    "\n",
    "    # actually grant permissions if necessary\n",
    "    if create_grant:\n",
    "        dbc.grants.update(\n",
    "            securable_type=catalog.SecurableType.CATALOG.name,\n",
    "            full_name=catalog_name,\n",
    "            changes=[\n",
    "                catalog.PermissionsChange(\n",
    "                    add=[catalog.Privilege.ALL_PRIVILEGES],\n",
    "                    principal=principal,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "# Create the admin schema, if it doesn't already exist\n",
    "try:\n",
    "    _ = dbc.schemas.get(f\"{catalog_name}.admin\")\n",
    "except NotFound:\n",
    "    dbc.schemas.create(\"admin\",catalog_name)\n",
    "\n",
    "# Grant permission on the admin schema to the service principal if required\n",
    "grants = dbc.grants.get(\n",
    "        securable_type=catalog.SecurableType.SCHEMA.name,\n",
    "        full_name=f\"{catalog_name}.admin\",\n",
    "        principal=service_principal_application_id,\n",
    "    ).privilege_assignments\n",
    "\n",
    "create_grant = True\n",
    "for pa in grants:\n",
    "    if catalog.Privilege.ALL_PRIVILEGES in pa.privileges:\n",
    "        create_grant = False\n",
    "\n",
    "# actually grant permissions if necessary\n",
    "if create_grant:\n",
    "    dbc.grants.update(\n",
    "        securable_type=catalog.SecurableType.SCHEMA.name,\n",
    "        full_name=f\"{catalog_name}.admin\",\n",
    "        changes=[\n",
    "            catalog.PermissionsChange(\n",
    "                add=[catalog.Privilege.ALL_PRIVILEGES],\n",
    "                principal=service_principal_application_id,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Create a limited view in the admin schema that DASL can use to track\n",
    "# the usage of DASL jobs.\n",
    "# This is needed for cost attribution and optimization features\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE VIEW `{catalog_name}`.admin.usage\n",
    "              AS SELECT record_id, sku_name,\n",
    "                 usage_start_time, usage_end_time, usage_date,\n",
    "                 usage_unit, usage_quantity, usage_type, ingestion_date,\n",
    "                 to_json(usage_metadata) as usage_metadata\n",
    "              FROM system.billing.usage\n",
    "              WHERE custom_tags.Source = 'DASL'\n",
    "              \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Small SQL warehouse that is used by the DASL UI and for some\n",
    "# report generation features. The vast majority of data processing is done\n",
    "# as serverless jobs, but nevertheless, this warehouse is required\n",
    "dasl_warehouses = [w for w in dbc.warehouses.list() if w.name == \"DASL Warehouse\"]\n",
    "if len(dasl_warehouses) == 0:\n",
    "    dasl_warehouse = dbc.warehouses.create(\n",
    "        enable_serverless_compute=True,\n",
    "        auto_stop_mins=10,\n",
    "        max_num_clusters=1,\n",
    "        cluster_size=\"Small\",\n",
    "        name=\"DASL Warehouse\",\n",
    "        tags=sql.EndpointTags(custom_tags=[sql.EndpointTagPair(key=\"Source\", value=\"DASL\")])\n",
    "        )\n",
    "else:\n",
    "    dasl_warehouse = dasl_warehouses[0]\n",
    "\n",
    "# Let the service principal and all users use this warehouse\n",
    "dbc.permissions.set(\n",
    "    request_object_type='warehouses',\n",
    "    request_object_id=dasl_warehouse.id,\n",
    "    access_control_list=[\n",
    "        iam.AccessControlRequest(\n",
    "            user_name=service_principal_application_id,\n",
    "            permission_level=iam.PermissionLevel.CAN_USE\n",
    "        ),\n",
    "        iam.AccessControlRequest(\n",
    "            group_name=\"users\",\n",
    "            permission_level=iam.PermissionLevel.CAN_USE\n",
    "        )]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the workspace has IP Access Lists in use and add the DASL control plane\n",
    "try:\n",
    "    conf_value = dbc.workspace_conf.get_status(keys=[\"enableIpAccessLists\"]).get(\"enableIpAccessLists\", \"false\")\n",
    "    ip_access_list_enabled = (False if conf_value is None or conf_value.lower() != \"true\" else True)\n",
    "    allow_lists = [l for l in dbc.ip_access_lists.list() if l.enabled==True\n",
    "                   and l.list_type == settings.ListType.ALLOW\n",
    "                   and l.label != \"DASL\"]\n",
    "    # Only add the DASL allow list if the workspace has IP access lists enabled and there is at least one\n",
    "    # existing enabled allow list\n",
    "    if ip_access_list_enabled and len(allow_lists) > 0:\n",
    "        print (\"Workspace appears to use IP allow lists. Adding DASL to a new allow list\")\n",
    "        # create the allow list if it doesn't already exist\n",
    "        if len([l for l in dbc.ip_access_lists.list() if l.label == \"DASL\"]) == 0:\n",
    "            dbc.ip_access_lists.create(\n",
    "                label='DASL',\n",
    "                ip_addresses=[\n",
    "                    \"35.170.18.183/32\",\n",
    "                    \"35.174.48.221/32\",\n",
    "                    \"3.234.73.41/32\"\n",
    "                ],\n",
    "                list_type=settings.ListType.ALLOW)\n",
    "    else:\n",
    "        print (\"Workspace does not appear to use IP allow lists\")\n",
    "except NotFound as e:\n",
    "    print(\"This workspace does not have IP access lists, nothing to be done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Databricks setup is now complete. Below, we create and set up the security lakehouse\n",
    "# workspace by making API calls to the control plane.\n",
    "\n",
    "am_workspace = Client.new_or_existing(\n",
    "    current_user,\n",
    "    app_client_id,\n",
    "    service_principal_application_id,\n",
    "    service_principal_secret,\n",
    "    workspace_url=f\"https://{databricks_instance}\",\n",
    "    dasl_host=\"https://api.sl.us-east-1.cloud.databricks.com\",\n",
    ")\n",
    "\n",
    "# This is a good initial config for DASL, but everything here can be\n",
    "# customized in the UI or through the API later\n",
    "am_workspace.put_config(\n",
    "    WorkspaceConfig(\n",
    "        dasl_storage_path=f\"/Volumes/{catalog_name}/internal/common\",\n",
    "        datasources=DatasourcesConfig(\n",
    "            catalog_name=catalog_name,\n",
    "            bronze_schema=\"bronze\",\n",
    "            silver_schema=\"silver\",\n",
    "            gold_schema=\"gold\",\n",
    "        ),\n",
    "        default_sql_warehouse=\"DASL Warehouse\",\n",
    "        detection_rule_metadata=DetectionRuleMetadata(\n",
    "            detection_categories=[\n",
    "                \"APT\",\n",
    "                \"Malware\",\n",
    "                \"Policy\",\n",
    "                \"SpecialEvent\",\n",
    "                \"SuspectEvent\",\n",
    "                \"Target\",\n",
    "                \"Trend\",\n",
    "            ],\n",
    "        ),\n",
    "        observables=WorkspaceConfigObservables(\n",
    "            kinds=[\n",
    "                WorkspaceConfigObservables.ObservablesKinds(\n",
    "                    name=\"Email Address\",\n",
    "                    sql_type=\"STRING\",\n",
    "                ),\n",
    "                WorkspaceConfigObservables.ObservablesKinds(\n",
    "                    name=\"IP Address\",\n",
    "                    sql_type=\"STRING\",\n",
    "                ),\n",
    "                WorkspaceConfigObservables.ObservablesKinds(\n",
    "                    name=\"Domain Name\",\n",
    "                    sql_type=\"STRING\",\n",
    "                ),\n",
    "            ],\n",
    "            relationships=[\n",
    "                \"ActingUser\",\n",
    "                \"DstIP\",\n",
    "                \"SrcIP\",\n",
    "                \"TargetUser\",\n",
    "            ],\n",
    "        ),\n",
    "        system_tables_config=SystemTablesConfig(\n",
    "            catalog_name=catalog_name,\n",
    "            var_schema=\"system\",\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DASL installation is now complete. A job will be created in your environment, called\n",
    "# 'dasl-background-tasks', that will create all the OCSF gold tables. This job takes\n",
    "# approximately 30-40 minutes to complete. After that has finished, you can navigate to\n",
    "# https://ui.sl.us-east-1.cloud.databricks.com, paste in a workspace URL, and use the application\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "dasl-client==1.0.26",
     "databricks-sdk==0.57.0"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DASL Install",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
